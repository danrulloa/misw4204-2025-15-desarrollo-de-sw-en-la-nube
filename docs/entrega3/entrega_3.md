# Arquitectura Ajustada

## Diagrama de Despliegue

![diagrama_despliegue_entrega3](https://private-user-images.githubusercontent.com/133672317/511690812-c1310327-11ea-46f7-bc35-f0a46bf2f707.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjI2MjE4NDYsIm5iZiI6MTc2MjYyMTU0NiwicGF0aCI6Ii8xMzM2NzIzMTcvNTExNjkwODEyLWMxMzEwMzI3LTExZWEtNDZmNy1iYzM1LWYwYTQ2YmYyZjcwNy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEwOFQxNzA1NDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02MDA4ZDlhZDdiYWVmMjhlY2VhYjJjYmJjNzE4YjNmNTM3Yzg5MzkwZjUxM2RlZmQzYTAzYWUzNmQ1M2VhMmQ1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.5bisduKen3CobMy47CdtkEb7B6tzAKkciXWMF6Mv8_o)

## Diagrama de Componentes

![diagrama_componentes_entrega3](https://private-user-images.githubusercontent.com/133672317/511692994-053c07a1-7278-4217-bc4d-e7606be962fa.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjI2MjE4NzIsIm5iZiI6MTc2MjYyMTU3MiwicGF0aCI6Ii8xMzM2NzIzMTcvNTExNjkyOTk0LTA1M2MwN2ExLTcyNzgtNDIxNy1iYzRkLWU3NjA2YmU5NjJmYS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEwOFQxNzA2MTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03MjU4YzVjODY0ZTYwMTQwYjM1ZTljM2Y1M2EzOGIzZTljMTI2MmYwMmIyM2Y0YTM5NmY2YTc4NTMzYmJhMmZlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.lNS1g7IP4UDAm7v_A3aYI9qjX4YQCdvNgt0f7zjYkyA)

## Diagrama de Flujo

![diagrama_flujo_entrega3](https://private-user-images.githubusercontent.com/133672317/511693764-bb7e76f9-d3d7-4169-a85f-72541a73e04b.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjI2MjE4NzIsIm5iZiI6MTc2MjYyMTU3MiwicGF0aCI6Ii8xMzM2NzIzMTcvNTExNjkzNzY0LWJiN2U3NmY5LWQzZDctNDE2OS1hODVmLTcyNTQxYTczZTA0Yi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEwOFQxNzA2MTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01NDA0NWU3ZWUyYjU5YWQ4MGQ4NjMzZWZhYjNkMzNkZDE4OTY3MmNiMmYzYWJjYjk2MTQ0MzhlODFkMjQxYjYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.n1gep-9Ul6OyBpDluOQMt8mGWBOLCU0mLM8oc9tQcF8)



# Explicación de Tecnologías y Servicios AWS y Otros Incorporados

La arquitectura de la Entrega 3 implementa una solución de microservicios distribuida en AWS que incorpora escalabilidad automática, servicios gestionados y observabilidad integral. A continuación se detallan las tecnologías y servicios implementados.

## Application Load Balancer (ALB)

El ALB actúa como punto de entrada único para todo el tráfico HTTP externo, operando en la capa 7 del modelo OSI. Distribuye las peticiones entrantes hacia múltiples servicios backend mediante reglas de enrutamiento basadas en rutas. El balanceador está configurado en modo internet-facing, desplegado en dos zonas de disponibilidad (us-east-1a y us-east-1b) para proporcionar alta disponibilidad.

Las reglas de enrutamiento dirigen `/api/*` hacia el Auto Scaling Group del Core API, `/auth/*` hacia el servicio de autenticación, `/grafana/*` hacia la interfaz de Grafana, `/rabbitmq/*` hacia la consola de administración de RabbitMQ y `/prometheus/*` hacia Prometheus. Cada target group tiene configurado un health check específico que verifica el estado de salud de las instancias registradas antes de dirigirles tráfico.

## Auto Scaling Group (ASG)

El ASG gestiona la escalabilidad automática del Core API, manteniendo entre 1 y 3 instancias EC2 tipo t3.small según la demanda. La política de escalado implementada es Target Tracking basada en utilización promedio de CPU con un objetivo del 60%. Cuando el uso de CPU supera este umbral de manera sostenida, el ASG aprovisiona instancias adicionales automáticamente. Cuando la carga disminuye, las instancias excedentes se terminan gradualmente, manteniendo siempre al menos una instancia activa.

El health check type está configurado como ELB, lo que significa que el ALB determina el estado de salud de cada instancia consultando el endpoint `/api/health`. Las instancias que fallan los health checks son reemplazadas automáticamente. El grace period de 300 segundos permite que las nuevas instancias completen su inicialización antes de recibir tráfico.

## Amazon RDS PostgreSQL

Se implementaron dos instancias separadas de Amazon RDS ejecutando PostgreSQL, ambas de tipo db.t3.micro. La primera instancia (anb-core-rds) almacena los datos del Core API incluyendo metadatos de videos, usuarios, votos y rankings. La segunda instancia (anb-auth-rds) se dedica exclusivamente a datos de autenticación como credenciales de usuario y sesiones activas.

Cada instancia RDS tiene configurado almacenamiento auto-escalable que comienza en 20 GB y puede crecer automáticamente hasta 100 GB según la demanda. Los backups automáticos se retienen durante 7 días, permitiendo point-in-time recovery. La encriptación en reposo está habilitada usando AES256. Dado el entorno educativo de AWS Academy y consideraciones de costo, Multi-AZ está deshabilitado.

## Amazon S3

El almacenamiento de archivos migró completamente a Amazon S3 utilizando un bucket único organizado en tres prefijos lógicos. El prefijo `/uploads/` almacena videos originales subidos por usuarios, `/processed/` contiene videos procesados por el worker y `/assets/` guarda recursos estáticos como watermarks e intros/outros.

El bucket tiene versionado habilitado, protegiendo contra eliminaciones accidentales. La encriptación server-side con AES256 está configurada por defecto. El acceso público está bloqueado, requiriendo que todas las operaciones se realicen mediante credenciales AWS. Las instancias EC2 acceden a S3 usando credenciales temporales de AWS Academy configuradas como variables de entorno.

## RabbitMQ

RabbitMQ se ejecuta en una instancia EC2 dedicada tipo t3.small y actúa como message broker para el procesamiento asíncrono. Expone tres puertos: 5672 para AMQP (consumido por Core API y Worker), 15672 para la interfaz de administración web y 15692 para métricas de Prometheus.

La configuración incluye dos colas: `video_processing` para el procesamiento principal y `video_processing.dlq` como dead-letter queue para mensajes que fallan repetidamente. Los plugins habilitados incluyen `rabbitmq_management` para la interfaz web y `rabbitmq_prometheus` para exportar métricas.

## Worker de Celery

El worker se ejecuta en una instancia EC2 tipo t3.large, seleccionada por los requisitos computacionales de FFmpeg. Procesa videos de manera asíncrona consumiendo tareas desde RabbitMQ. El flujo de procesamiento descarga el video original desde S3, lo procesa localmente con FFmpeg (redimensionamiento a 720p, conversión a H.264, compresión, agregado de watermark y concatenación de intro/outro), sube el resultado procesado a S3 y actualiza el estado en RDS.

El worker mantiene concurrencia de 1 tarea a la vez para evitar saturar la CPU con múltiples procesos FFmpeg simultáneos. Si una tarea falla, RabbitMQ la reintenta automáticamente o la redirige a la dead-letter queue después de varios intentos fallidos.

## Stack de Observabilidad

La observabilidad se implementa mediante un stack híbrido que combina herramientas open-source con servicios de AWS. Prometheus scrapea métricas de Auth Service (puerto 8001), RabbitMQ (puerto 15692) y Worker (puerto 9100). Las instancias del Core API en el ASG no se scrapean directamente debido a sus IPs dinámicas; en su lugar, se utilizan métricas de CloudWatch.

Grafana proporciona dashboards que visualizan métricas de Prometheus y CloudWatch en una interfaz unificada. Loki agrega logs recolectados por Promtail de todas las instancias EC2. CloudWatch complementa el stack recolectando logs y métricas de todas las instancias EC2, las instancias RDS y el ALB.

## Infraestructura de Red

La arquitectura se despliega en una VPC dedicada (10.0.0.0/16) distribuida en dos zonas de disponibilidad. Se utilizan únicamente subredes públicas (10.0.1.0/24 en us-east-1a y 10.0.2.0/24 en us-east-1b) conectadas directamente a Internet mediante un Internet Gateway. Esta decisión simplifica la configuración en el entorno de AWS Academy, aunque en producción se recomendaría usar subredes privadas con NAT Gateway.

Los security groups implementan el principio de least privilege, permitiendo únicamente el tráfico necesario entre componentes. Por ejemplo, el security group de RDS permite conexiones en el puerto 5432 solo desde Core API, Auth Service, Worker y administradores autorizados.

## Gestión de Credenciales

Dado que AWS Academy no proporciona acceso completo a IAM, la arquitectura utiliza credenciales temporales de sesión que incluyen Access Key, Secret Key y Session Token. Estas credenciales se configuran localmente y Terraform las inyecta en las instancias EC2 mediante el mecanismo de user-data, estableciéndolas como variables de entorno.

Las aplicaciones acceden a S3 usando boto3, que detecta automáticamente estas credenciales desde las variables de entorno. Las credenciales de RDS (usuario y contraseña) se gestionan independientemente y se configuran durante la creación de las instancias RDS, permaneciendo válidas incluso cuando las credenciales de AWS expiran.

# Cambios Realizados con Respecto a Entrega 2

La Entrega 3 representa una evolución significativa respecto a la arquitectura de la Entrega 2, introduciendo servicios gestionados de AWS, escalabilidad automática y almacenamiento en la nube.

## Eliminación de Componentes

### Instancia Web con Nginx

En la Entrega 2, se utilizaba una instancia EC2 dedicada ejecutando Nginx como reverse proxy y balanceador de carga. Esta instancia recibía todo el tráfico externo y lo distribuía hacia las instancias backend según reglas configuradas estáticamente en `nginx.conf`. La configuración requería IPs privadas fijas y no proporcionaba verdadera alta disponibilidad al ser un punto único de falla.

Esta instancia fue completamente eliminada y reemplazada por el Application Load Balancer de AWS, un servicio gestionado que proporciona distribución de tráfico inteligente, health checks avanzados y alta disponibilidad multi-AZ sin necesidad de gestión manual.

### Instancia de Base de Datos

La Entrega 2 implementaba las bases de datos PostgreSQL como contenedores Docker ejecutándose en una instancia EC2 dedicada. Se utilizaban dos contenedores: `anb-core-db` en el puerto 5432 y `anb-auth-db` en el puerto 5433, con volúmenes Docker para persistencia. Esta configuración requería gestión manual de backups, actualizaciones y mantenimiento.

Ambos contenedores fueron eliminados y reemplazados por dos instancias Amazon RDS PostgreSQL gestionadas, que proporcionan backups automáticos con 7 días de retención, patching automático, encriptación en reposo y almacenamiento auto-escalable.

## Incorporación de Servicios AWS

### Application Load Balancer

El ALB se configuró con listeners en puerto 80 (HTTP) y múltiples target groups para cada servicio backend. Las reglas de enrutamiento basadas en path permiten consolidar múltiples servicios detrás de un único DNS público. El ALB realiza health checks continuos consultando endpoints específicos (`/api/health` para Core API, `/auth/api/v1/status` para Auth Service) y elimina automáticamente del pool de distribución las instancias que fallan.

La configuración multi-AZ del ALB distribuye automáticamente el tráfico entre instancias disponibles en ambas zonas de disponibilidad, proporcionando resiliencia ante fallos de una AZ completa.

### Auto Scaling Group

El Core API, que en la Entrega 2 era una instancia EC2 única y fija, ahora se gestiona mediante un Auto Scaling Group que mantiene entre 1 y 3 instancias según la carga. La política de Target Tracking monitorea continuamente la utilización promedio de CPU de todas las instancias activas. Cuando este promedio supera el 60% de manera sostenida, el ASG lanza automáticamente instancias adicionales.

El Launch Template define la configuración de cada nueva instancia, incluyendo el AMI de Ubuntu 22.04, el tipo de instancia t3.small, el security group apropiado y el script de user-data que instala Docker, clona el repositorio y levanta los servicios necesarios. Las nuevas instancias se registran automáticamente en el target group del ALB.

### Amazon RDS

Las dos instancias RDS (anb-core-rds y anb-auth-rds) eliminan la necesidad de gestionar bases de datos manualmente. El servicio RDS maneja automáticamente tareas operacionales como backups, restauración point-in-time, aplicación de parches de seguridad y actualizaciones de versión.

El almacenamiento auto-escalable comienza en 20 GB y puede crecer automáticamente hasta 100 GB según la demanda, eliminando la necesidad de planificar capacidad con anticipación. La encriptación en reposo protege los datos sensibles almacenados en las bases de datos.

### Amazon S3

El almacenamiento de videos migró completamente desde volúmenes EBS locales hacia Amazon S3. Esta transición elimina las limitaciones de capacidad fija de los volúmenes EBS y proporciona durabilidad del 99.999999999% con redundancia automática en múltiples AZ.

El bucket S3 permite almacenar un número ilimitado de archivos con costos basados únicamente en uso real, no en capacidad aprovisionada. El versionado habilitado protege contra eliminaciones accidentales, manteniendo versiones anteriores de los archivos para recuperación.

## Modificaciones en Código

### Core API: Storage Adapter

Se implementó el patrón Storage Adapter mediante una factory function `get_storage()` que retorna diferentes implementaciones según la configuración. En desarrollo local, retorna `LocalStorageAdapter` que opera sobre el sistema de archivos. En producción AWS, retorna `S3StorageAdapter` que utiliza boto3 para interactuar con S3.

La interfaz `StoragePort` define los métodos `save()`, `load()` y `delete()` que ambos adapters implementan, permitiendo que el resto del código permanezca sin cambios independientemente del backend de almacenamiento utilizado. Cuando se guarda un archivo, el S3StorageAdapter genera una ruta S3 con el patrón `/uploads/YYYY/MM/DD/uuid-filename.mp4` y realiza un PUT del contenido al bucket configurado.

### Worker: Detección y Manejo de S3

El worker fue modificado para detectar si las rutas de entrada y salida corresponden a archivos S3 o locales. Se implementó la función `_is_s3_path()` que identifica rutas S3 mediante la presencia del prefijo `s3://` o la referencia al bucket configurado.

Cuando el worker recibe una tarea, primero descarga el video original desde S3 a un archivo temporal en `/tmp/`. Después del procesamiento con FFmpeg, sube el video procesado de vuelta a S3 bajo el prefijo `/processed/`. Esta arquitectura mantiene el procesamiento de FFmpeg completamente local, minimizando transferencias de red durante la conversión.

### Configuración de Variables de Entorno

El script de user-data (`userdata.sh.tftpl`) fue actualizado para inyectar dinámicamente los endpoints de RDS, el nombre del bucket S3 y las credenciales AWS en el archivo `.env` de cada instancia. Terraform pasa estas variables como template variables, permitiendo que cada instancia se configure automáticamente sin intervención manual.

Las instancias del Core API reciben `DATABASE_URL` apuntando a anb-core-rds, `STORAGE_BACKEND=s3`, `S3_BUCKET` con el nombre del bucket, y las credenciales AWS temporales. El Auth Service recibe configuración similar pero apuntando a anb-auth-rds. El Worker recibe endpoints tanto de RDS como de S3 para leer metadatos y gestionar archivos.

## Cambios Operacionales

### Despliegue

La Entrega 3 automatiza completamente el despliegue mediante Terraform. El comando `terraform apply` crea toda la infraestructura, incluyendo VPC, subnets, security groups, ALB, ASG, RDS y S3. Las actualizaciones del Core API se realizan modificando el Launch Template y ejecutando un instance refresh del ASG, que reemplaza gradualmente las instancias antiguas con nuevas mientras mantiene la capacidad mínima.

### Escalabilidad

La Entrega 3 escala automáticamente. Cuando la CPU promedio del ASG supera el 60%, una nueva instancia se lanza automáticamente, se configura mediante user-data, se registra en el ALB target group y comienza a recibir tráfico sin intervención humana. El proceso completo toma aproximadamente 5 minutos desde la detección del umbral hasta que la nueva instancia acepta peticiones.

### Recuperación ante Fallos

Si una instancia del Core API falla en la Entrega 2, el servicio quedaba completamente inoperativo hasta que un administrador detectara el problema.

En la Entrega 3, el ALB detecta instancias no saludables mediante health checks cada 30 segundos. Si una instancia falla tres health checks consecutivos, el ALB deja de enviarle tráfico inmediatamente. Simultáneamente, el ASG detecta la instancia no saludable y la reemplaza automáticamente con una nueva, todo sin intervención manual y sin downtime perceptible para los usuarios.

### Observabilidad

La Entrega 2 configuraba Prometheus con targets estáticos basados en IPs privadas fijas. Esta configuración funcionaba correctamente pero requería actualizaciones manuales al agregar o eliminar instancias.

La Entrega 3 mantiene Prometheus para instancias fijas (Auth, Worker, RabbitMQ) pero complementa con CloudWatch para las instancias del ASG que tienen IPs dinámicas. CloudWatch recolecta automáticamente métricas de CPU, red, disco y métricas custom de todas las instancias del ASG sin necesidad de configuración de service discovery. Grafana integra ambas fuentes de métricas en dashboards unificados.
