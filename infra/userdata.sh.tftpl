#!/bin/bash
set -euo pipefail
# Log a archivo y consola para facilitar troubleshooting de bootstrap
exec > >(tee -a /var/log/anb-bootstrap.log) 2>&1

ROLE="${role}"
REPO_URL="${repo_url}"
repo_branch="${repo_branch}"
COMPOSE_FILE="${compose_file}"

core_ip="${core_ip}"
db_ip="${db_ip}"
mq_ip="${mq_ip}"
obs_ip="${obs_ip}"
web_ip="${web_ip}"
worker_ip="${worker_ip}"
alb_dns="${alb_dns}"

# RDS endpoints (vacíos si no se usan RDS)
rds_core_endpoint="${rds_core_endpoint}"
rds_auth_endpoint="${rds_auth_endpoint}"
rds_password="${rds_password}"

# S3 bucket (vacío si no se usa S3)
s3_bucket="${s3_bucket}"
aws_access_key_id="${aws_access_key_id}"
aws_secret_access_key="${aws_secret_access_key}"
aws_session_token="${aws_session_token}"
aws_region="${aws_region}"
assets_inout_key="${assets_inout_key}"
assets_wm_key="${assets_wm_key}"

apt-get update -y
apt-get install -y ca-certificates curl gnupg lsb-release git
install -m 0755 -d /etc/apt/keyrings || true
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
https://download.docker.com/linux/ubuntu $(. /etc/os-release; echo "$VERSION_CODENAME") stable" \
> /etc/apt/sources.list.d/docker.list
apt-get update -y
apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
systemctl enable --now docker
usermod -aG docker ubuntu || true

mkdir -p /opt/anb-cloud
cd /opt/anb-cloud
if [ ! -d .git ]; then
  git clone "$REPO_URL" /opt/anb-cloud
fi
git fetch --all
git checkout "$repo_branch"
git reset --hard "origin/${repo_branch}"

# ----- .env por rol -----
# Configuración estricta: requerimos ALB DNS; sin ALB no continuamos
if [ -z "$alb_dns" ]; then
  echo "ERROR: alb_dns no está definido. Terraform debe pasar el DNS del ALB." >&2
  exit 1
fi
LOKI_URL_VALUE="http://$alb_dns/loki/api/v1/push"

cat > /opt/anb-cloud/.env <<EOF
APP_ENV=production
API_TAG=latest

# CORE & AUTH DBs
POSTGRES_USER=anb_user
POSTGRES_PASSWORD=anb_pass
POSTGRES_CORE_DB=anb_core
POSTGRES_AUTH_DB=anb_auth

ACCESS_TOKEN_SECRET_KEY=mi_clave_de_acceso_secreta
REFRESH_TOKEN_SECRET_KEY=mi_clave_de_refresh_secreta
TOKEN_EXPIRE=600
REFRESH_TOKEN_EXPIRE=600
# RDS o PostgreSQL local según disponibilidad
%{ if rds_core_endpoint != "" ~}
DB_URL_CORE=postgresql+asyncpg://anb_user:${rds_password}@${rds_core_endpoint}:5432/anb_core
DB_URL_AUTH=postgresql+asyncpg://anb_user:${rds_password}@${rds_auth_endpoint}:5432/anb_auth
DATABASE_URL=postgresql://anb_user:${rds_password}@${rds_core_endpoint}:5432/anb_core
%{ else ~}
DB_URL_CORE=postgresql+asyncpg://anb_user:anb_pass@${db_ip}:5432/anb_core
DB_URL_AUTH=postgresql+asyncpg://anb_user:anb_pass@${db_ip}:5433/anb_auth
DATABASE_URL=postgresql://anb_user:anb_pass@${db_ip}:5432/anb_core
%{ endif ~}

# MQ
RABBITMQ_DEFAULT_USER=rabbit
RABBITMQ_DEFAULT_PASS=rabbitpass
RABBITMQ_PORT=5672
RABBITMQ_VHOST=/
RABBITMQ_URL=amqp://rabbit:rabbitpass@${mq_ip}:5672/%2F
RABBITMQ_HOST=${mq_ip}

# WEB upstreams
UPSTREAM_API=http://${core_ip}:8000
UPSTREAM_AUTH=http://${core_ip}:8001

# JWT & Security
JWT_SECRET=mi_secreto_super_seguro_para_jwt_tokens_2024
ALGORITHM=HS256

# S3 Configuration (si bucket está definido)
%{ if s3_bucket != "" ~}
STORAGE_BACKEND=s3
S3_BUCKET=${s3_bucket}
S3_REGION=${aws_region}
S3_PREFIX=uploads
S3_FORCE_PATH_STYLE=0
S3_VERIFY_SSL=1
# Credenciales explícitas para S3 (fail-fast en la app si faltan)
AWS_ACCESS_KEY_ID=${aws_access_key_id}
AWS_SECRET_ACCESS_KEY=${aws_secret_access_key}
AWS_SESSION_TOKEN=${aws_session_token}
%{ else ~}
STORAGE_BACKEND=local
%{ endif ~}
EOF

# Exportar LOKI_URL sólo en el entorno de la sesión (no en .env)
export LOKI_URL="$LOKI_URL_VALUE"

# Configurar prometheus con IPs dinámicas solo en OBS
if [ "$ROLE" = "obs" ]; then
  # External URL para que la UI de Prometheus genere enlaces con el prefijo /prometheus
  if [ -n "$alb_dns" ]; then
    echo "PROMETHEUS_EXTERNAL_URL=http://$alb_dns/prometheus" >> /opt/anb-cloud/.env
  fi
  # Usar el archivo base y sustituir placeholders con IPs reales
  # Evitar ciclos: solo sustituir WEB_IP si viene no vacío
  if [ -n "${web_ip}" ]; then
    sed -i "s/__WEB_IP__/${web_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  fi
  sed -i "s/__CORE_IP__/${core_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  sed -i "s/__DB_IP__/${db_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  sed -i "s/__MQ_IP__/${mq_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  sed -i "s/__WORKER_IP__/${worker_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  # Si no recibimos obs_ip desde Terraform, consultamos a IMDS la IP privada
  if [ -z "${obs_ip}" ]; then
    obs_ip_runtime="$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4 || true)"
  else
    obs_ip_runtime="${obs_ip}"
  fi
  if [ -n "$${obs_ip_runtime}" ]; then
    sed -i "s/__OBS_IP__/$${obs_ip_runtime}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  fi
  # Si quedaron entradas con __WEB_IP__ (no hay VM WEB), elimínalas para evitar targets inválidos
  sed -i '/__WEB_IP__/d' /opt/anb-cloud/observability/prometheus/prometheus.yml || true
fi

# Configurar nginx con IPs dinámicas solo en WEB
if [ "$ROLE" = "web" ]; then
  # Generar nginx.conf con las IPs correctas
  sed -i "s/__CORE_IP__/${core_ip}/g" /opt/anb-cloud/nginx/nginx.conf
  sed -i "s/__MQ_IP__/${mq_ip}/g" /opt/anb-cloud/nginx/nginx.conf  
  sed -i "s/__OBS_IP__/${obs_ip}/g" /opt/anb-cloud/nginx/nginx.conf
fi

# ----- Arranque por perfil -----
# Si el rol es worker y hay bucket S3 definido, sincronizar assets locales desde S3
if [ "$ROLE" = "worker" ]; then
  : # no-op para evitar bloque vacío si el template no inyecta contenido
  %{ if s3_bucket != "" ~}
  echo "Sincronizando assets del worker desde S3 (obligatorio)..."
  apt-get install -y awscli || true
  export AWS_ACCESS_KEY_ID="${aws_access_key_id}"
  export AWS_SECRET_ACCESS_KEY="${aws_secret_access_key}"
  export AWS_SESSION_TOKEN="${aws_session_token}"
  export AWS_DEFAULT_REGION="${aws_region}"
  mkdir -p /opt/anb-cloud/worker/assets
  # Descarga obligatoria de assets; falla si no existen en S3
  aws s3 cp "s3://${s3_bucket}/${assets_inout_key}" \
    /opt/anb-cloud/worker/assets/inout.mp4 --region "${aws_region}"
  aws s3 cp "s3://${s3_bucket}/${assets_wm_key}" \
    /opt/anb-cloud/worker/assets/watermark.png --region "${aws_region}"
  ls -lh /opt/anb-cloud/worker/assets || true
  %{ endif ~}
fi

# Descubrir archivo compose de forma robusta
COMPOSE_PATH=""
if [ -f "$COMPOSE_FILE" ]; then
  COMPOSE_PATH="$COMPOSE_FILE"
elif [ -f "docker-compose.multihost.yml" ]; then
  COMPOSE_PATH="docker-compose.multihost.yml"
elif [ -f "deploy/compose/docker-compose.multihost.yml" ]; then
  COMPOSE_PATH="deploy/compose/docker-compose.multihost.yml"
else
  echo "[anb] ERROR: No se encontró archivo compose. Probados: $COMPOSE_FILE, docker-compose.multihost.yml, deploy/compose/docker-compose.multihost.yml" >&2
  ls -la || true
  exit 1
fi
echo "[anb] Usando compose: $COMPOSE_PATH"

case "$ROLE" in
  web)    docker compose -f "$COMPOSE_PATH" --profile web    up -d ;;
  core)   docker compose -f "$COMPOSE_PATH" --profile core   up -d ;;
  db)     docker compose -f "$COMPOSE_PATH" --profile db     up -d ;;
  mq)     docker compose -f "$COMPOSE_PATH" --profile mq     up -d ;;
  worker) docker compose -f "$COMPOSE_PATH" --profile worker up -d ;;
  obs)    docker compose -f "$COMPOSE_PATH" --profile obs    up -d ;;
esac

docker compose -f "$COMPOSE_PATH" ps || true

