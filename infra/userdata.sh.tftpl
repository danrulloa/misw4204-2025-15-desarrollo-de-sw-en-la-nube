#!/bin/bash
set -euo pipefail
# Log a archivo y consola
exec > >(tee -a /var/log/anb-bootstrap.log) 2>&1

# ====== Variables del template (Terraform) ======
ROLE="${role}"
REPO_URL="${repo_url}"
repo_branch="${repo_branch}"
COMPOSE_FILE="${compose_file}"

core_ip="${core_ip}"
auth_ip="${auth_ip}"
mq_ip="${mq_ip}"
obs_ip="${obs_ip}"
web_ip="${web_ip}"
worker_ip="${worker_ip}"
alb_dns="${alb_dns}"

# RDS endpoints (obligatorios)
rds_core_endpoint="${rds_core_endpoint}"
rds_auth_endpoint="${rds_auth_endpoint}"
rds_password="${rds_password}"

# S3 / Región, PERFIL y credenciales (si vienen del host que ejecuta Terraform)
s3_bucket="${s3_bucket}"
aws_region="${aws_region}"
aws_profile="${aws_profile}"
aws_access_key_id="${aws_access_key_id}"
aws_secret_access_key="${aws_secret_access_key}"
aws_session_token="${aws_session_token}"
assets_inout_key="${assets_inout_key}"
assets_wm_key="${assets_wm_key}"

# ====== Paquetes base + Docker ======
apt-get update -y
apt-get install -y ca-certificates curl gnupg lsb-release git awscli
install -m 0755 -d /etc/apt/keyrings || true
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
https://download.docker.com/linux/ubuntu $(. /etc/os-release; echo "$VERSION_CODENAME") stable" \
> /etc/apt/sources.list.d/docker.list
apt-get update -y
apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
systemctl enable --now docker
usermod -aG docker ubuntu || true

# ====== Código fuente ======
mkdir -p /opt/anb-cloud
cd /opt/anb-cloud
if [ ! -d .git ]; then
  git clone "$REPO_URL" /opt/anb-cloud
fi
git fetch --all || true
git checkout "$repo_branch"
git reset --hard "origin/${repo_branch}"

# ====== Validaciones mínimas ======
if [ -z "$alb_dns" ]; then
  echo "ERROR: alb_dns no está definido (DNS del ALB requerido)." >&2
  exit 1
fi
LOKI_URL_VALUE="http://$alb_dns/loki/api/v1/push"

# Requerir bucket S3 siempre (no hay almacenamiento local)
if [ -z "$s3_bucket" ]; then
  echo "ERROR: s3_bucket no está definido y el almacenamiento local ya no está soportado." >&2
  exit 1
fi

# (OTLP/Tempo removido) No se requiere resolución de OBS para tracing

# ====== .env (S3 obligatorio; incluye credenciales AWS si fueron provistas) ======
cat > /opt/anb-cloud/.env <<EOF
APP_ENV=production
API_TAG=latest

# DB (RDS obligatorio)
DB_URL_CORE=postgresql+asyncpg://anb_user:${rds_password}@${rds_core_endpoint}:5432/anb_core
DB_URL_AUTH=postgresql+asyncpg://anb_user:${rds_password}@${rds_auth_endpoint}:5432/anb_auth
DATABASE_URL=postgresql://anb_user:${rds_password}@${rds_core_endpoint}:5432/anb_core

# MQ
RABBITMQ_DEFAULT_USER=rabbit
RABBITMQ_DEFAULT_PASS=rabbitpass
RABBITMQ_PORT=5672
RABBITMQ_VHOST=/
RABBITMQ_URL=amqp://rabbit:rabbitpass@${mq_ip}:5672/%2F
RABBITMQ_HOST=${mq_ip}

# Upstreams (WEB)

# WEB upstreams
UPSTREAM_API=http://${core_ip}:8000
UPSTREAM_AUTH=http://${auth_ip}:8001

# JWT
ACCESS_TOKEN_SECRET_KEY=mi_clave_de_acceso_secreta
REFRESH_TOKEN_SECRET_KEY=mi_clave_de_refresh_secreta
TOKEN_EXPIRE=600
REFRESH_TOKEN_EXPIRE=600
JWT_SECRET=mi_secreto_super_seguro_para_jwt_tokens_2024
ALGORITHM=HS256

# S3 (obligatorio)
# Credenciales AWS (opcionales)
AWS_ACCESS_KEY_ID=${aws_access_key_id}
AWS_SECRET_ACCESS_KEY=${aws_secret_access_key}
AWS_SESSION_TOKEN=${aws_session_token}
%{ if aws_profile != "" ~}
AWS_PROFILE=${aws_profile}
%{ endif ~}
AWS_REGION=${aws_region}
AWS_SDK_LOAD_CONFIG=1
STORAGE_BACKEND=s3
S3_BUCKET=${s3_bucket}
S3_REGION=${aws_region}
S3_PREFIX=uploads
S3_FORCE_PATH_STYLE=0
S3_VERIFY_SSL=1

# Upload tuning (concurrency & S3 transfer)
# Ajustables vía Terraform en el futuro; por ahora dejamos valores recomendados por defecto
UPLOAD_IO_MAX_WORKERS=32
S3_MAX_POOL_CONNECTIONS=100
S3_TRANSFER_MAX_CONCURRENCY=12
EOF

# (OTLP/Tempo removido) No se configuran variables OTEL en .env

# Solo exportamos LOKI_URL en runtime (los contenedores lo leen por env/compose)
export LOKI_URL="$LOKI_URL_VALUE"

# (OTLP/Tempo removido) No se espera por endpoint de trazas

# ====== Prometheus (solo OBS) ======
if [ "$ROLE" = "obs" ]; then
  if [ -n "$alb_dns" ]; then
    echo "PROMETHEUS_EXTERNAL_URL=http://$alb_dns/prometheus" >> /opt/anb-cloud/.env
  fi

  if [ -n "${web_ip}" ]; then
    sed -i "s/__WEB_IP__/${web_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  fi
  # CORE ahora usa EC2 service discovery, no reemplazamos __CORE_IP__ (ya no existe en prometheus.yml)
  sed -i "s/__AUTH_IP__/${auth_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  sed -i "s/__MQ_IP__/${mq_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  sed -i "s/__WORKER_IP__/${worker_ip}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml

  if [ -z "${obs_ip}" ]; then
    obs_ip_runtime="$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4 || true)"
  else
    obs_ip_runtime="${obs_ip}"
  fi
  if [ -n "$obs_ip_runtime" ]; then
    sed -i "s/__OBS_IP__/$${obs_ip_runtime}/g" /opt/anb-cloud/observability/prometheus/prometheus.yml
  fi
  sed -i '/__WEB_IP__/d' /opt/anb-cloud/observability/prometheus/prometheus.yml || true
fi

# ====== Nginx (solo WEB) ======
if [ "$ROLE" = "web" ]; then
  sed -i "s/__CORE_IP__/${core_ip}/g" /opt/anb-cloud/nginx/nginx.conf
  sed -i "s/__MQ_IP__/${mq_ip}/g"   /opt/anb-cloud/nginx/nginx.conf
  sed -i "s/__OBS_IP__/${obs_ip}/g" /opt/anb-cloud/nginx/nginx.conf
fi

# ====== Assets Worker desde S3 (con credenciales explícitas o perfil/rol) ======
if [ "$ROLE" = "worker" ] && [ -n "$s3_bucket" ]; then
  echo "Descargando assets del worker desde S3..."
  apt-get install -y awscli || true

  # Preferir credenciales explícitas si vienen; si no, usar perfil; si no, rol de instancia (IMDS)
  if [ -n "$aws_access_key_id" ]; then
    export AWS_ACCESS_KEY_ID="$aws_access_key_id"
    export AWS_SECRET_ACCESS_KEY="$aws_secret_access_key"
    if [ -n "$aws_session_token" ]; then
      export AWS_SESSION_TOKEN="$aws_session_token"
    fi
    unset AWS_PROFILE || true
  elif [ -n "$aws_profile" ]; then
    export AWS_PROFILE="$aws_profile"
  fi
  export AWS_DEFAULT_REGION="${aws_region}"

  mkdir -p /opt/anb-cloud/worker/assets
  aws s3 cp "s3://${s3_bucket}/${assets_inout_key}" /opt/anb-cloud/worker/assets/inout.mp4
  aws s3 cp "s3://${s3_bucket}/${assets_wm_key}"    /opt/anb-cloud/worker/assets/watermark.png
  ls -lh /opt/anb-cloud/worker/assets || true
fi

# ====== Resolver ruta del compose ======
COMPOSE_PATH=""
if [ -f "$COMPOSE_FILE" ]; then
  COMPOSE_PATH="$COMPOSE_FILE"
elif [ -f "docker-compose.multihost.yml" ]; then
  COMPOSE_PATH="docker-compose.multihost.yml"
elif [ -f "deploy/compose/docker-compose.multihost.yml" ]; then
  COMPOSE_PATH="deploy/compose/docker-compose.multihost.yml"
else
  echo "[anb] ERROR: No se encontró archivo compose. Probados: $COMPOSE_FILE, docker-compose.multihost.yml, deploy/compose/docker-compose.multihost.yml" >&2
  ls -la || true
  exit 1
fi
echo "[anb] Usando compose: $COMPOSE_PATH"

# ====== Arranque por perfil ======
case "$ROLE" in
  web)    docker compose -f "$COMPOSE_PATH" --profile web    up -d ;;
  core)   docker compose -f "$COMPOSE_PATH" --profile core   up -d ;;
  auth)   docker compose -f "$COMPOSE_PATH" --profile auth   up -d ;;
  mq)     docker compose -f "$COMPOSE_PATH" --profile mq     up -d ;;
  worker) docker compose -f "$COMPOSE_PATH" --profile worker up -d ;;
  obs)    docker compose -f "$COMPOSE_PATH" --profile obs    up -d ;;
esac

docker compose -f "$COMPOSE_PATH" ps || true
